## Linear Combinations
### plain
Welcome to the geometry of linear algebra! Before we dive head-first into these topics, let's do a little check of your geometric intuitions!

### plain
You should know by now that vectors have two principal operations: vector addition, and scalar multiplication.

### mcq
Now suppose you're stuck in $\mathbb{R}^3$. Which of the following is the best \mathbf{interpretation} of $\left(\begin{matrix} 1 \\ 0 \\ 0 \end{matrix}\right) + 3\left(\begin{matrix} 0 \\ 1 \\ 0 \end{matrix}\right)$?
- I didn't study Chapters 3.1 \& 3.2 :(
- $\left(\begin{matrix} 1 \\ 1 \\ 0 \end{matrix}\right)$
* "Taking a step" (i.e. translating 1 unit) in the x-direction then "taking 3 steps" in the y-direction.

### plain
Exactly! We can think of both operations as defining how we may move within a (vector) space...

### dialogue
... except I lied. 
- Wait, what?
    - iLiedHaha

### plain iLiedHaha
Notice how the operations fundamentally build upon "anchoring" vectors to create new directions in which you can move within a vector space.

To see what I mean, riddle me this: 

### mcq
Clearly in $\mathbb{R}^3$ I can move along the z-axis. But using only $\left(\begin{matrix} 1 \\ 0 \\ 0 \end{matrix}\right)$ and $\left(\begin{matrix} 0 \\ 1 \\ 0 \end{matrix}\right)$, can our vector operations ever give us the vector $\left(\begin{matrix} 0 \\ 0 \\ 1 \end{matrix}\right)$?
- Yes
* No

### plain
Right, we can't. So it's not entirely correct to say that vector operations DETERMINE how we can move within a vector space. That's because the vectors we choose to add and multiply ALSO affect where we can move!

### plain 
Rather, vector operations give us the **mathematical structure** that shapes the RULES of where we can move.

### plain
With that in mind, let's introduce **linear combinations** to help us keep a mental note of this structure:

A linear combination of the (non-empty) set of vectors $\{\mathbf{u}_1, \dots, \mathbf{u}_k\}$ is any vector $\mathbf{v}$ satisfying 

### tex

$$\textbf{Linear Combination of }\{\mathbf{u}_1, \dots, \mathbf{u}_k\} \\ $$

$$\mathbf{v} = c_1 \mathbf{u}_1 + c_2 \mathbf{u}_2 + \dots + c_k \mathbf{u}_k, \newline c_1, \dots c_k \in \mathbb{R}$$

### plain
‚òùÔ∏èü§ì Erm... Ackchually... if you did the Interactive for Chapters 2.2~2.3 before, you would ALSO know that we can write $\mathbf{v}$ like this:

### tex
$$\textbf{Alternative Linear Combination} \\ $$

$$\mathbf{v} = \left(\begin{array}{c|c|c|c} \mathbf{u}_1 & \mathbf{u}_2 &  \dots &\mathbf{u}_k \end{array}\right) \left(\begin{array}{c} c_1 \\ c_2 \\ \vdots \\ c_k\end{array}\right) \\ $$


### plain
‚òùÔ∏èü§ì Um... This is a magical tool that will come in handy later...

## Spans
### dialogue
On the topic of mathematical structures, do you think that the set of linear combinations of any (non-empty) set of vectors form a vector space?

Formally, will $\{c_1 \mathbf{u}_1 + c_2 \mathbf{u}_2 + \dots + c_k \mathbf{u}_k, \mid c_1,\dots c_k \in \mathbb{R}\}$ always form a vector space (if I equip it with vector addition and scalar multiplication)?

- Yes
- No
- Just tell me the answer >:(
- Spare me ;-;

### plain
Unsurprisingly, it can! (Proof left as exercise for you :3)

### plain
This allows us to define vector spaces in the alternative manner below:

### tex
$$\textbf{Span of Vector Set} \\ $$

$$\text{For a set } S = \{\mathbf{u}_1,\dots,\mathbf{u}_k\}, 
\mathrm{span}(S)=\{c_1\mathbf{u}_1+\cdots+c_k\mathbf{u}_k \mid c_1,\dots c_k \in \mathbb{R}\}
$$

### dialogue
The vector space structure is exactly that provided to you by vector addition and scalar multiplication, preserving some of the basic intuitions one might have about space (the concept in general).
- Like what?
    - theWhat

### plain theWhat
In particular, **you can't move or stretch out of a space**, despite what the Backrooms lore tells you.
So no matter how many steps you can in whichever directions you desire, you will always stay within $span(S)$. Try it!

### dialogue
Therefore we say that spans **are (vector) spaces** and are therefore **closed under linear combinations**.
Vector spaces themselves are **ALSO** closed under linear combination.
- Interesting...
    - spansInDetail

### mcq
After all I've yapped, let's check if you get the key point:
Which of the following are properties of the span of a set of vectors in $\mathbb{R}^n$?
* $\mathbf{0}^{n \times 1}$ is in the span.
* If $\mathbf{u}, \mathbf{v}$ are in the span, so is $\mathbf{u} + \mathbf{v}$
* If $\mathbf{u}$ is in the span, so is $a\mathbf{u}$ for any non-zero real $a$.
* If $\mathbf{u}, \mathbf{v}$ are in the span, so is $a\mathbf{u} + b\mathbf{v}$ for reals $a, b$, not both zero.

### plain
üß†üí™ Great! (Technically, there is no need to enforce non-zero outside of this question.)

## A Closer Look at Spans
### plain spansInDetail
Let's explore spans in greater detail!

### dialogue
Have you ever wondered why we keep referring to vector spaces in the plural?
- Yes
    - continueVectorSpaces
- No
    - pretendToBeSad

### plain pretendToBeSad
Not curious? That makes me sad... but I'll tell you anyway! :)

### plain continueVectorSpaces
That's because **not all vector spaces are built the same**.

### mcq
Consider just $\mathbb{R}^3$, your usual 3D Euclidean space. How many lines and planes can we have in that Euclidean space which pass through the origin?
- Two
* Infinitely many
- None
- I refuse to answer your question

### plain

Already we have infinitely many lines (spaces in \mathbb{R}^3 spanned by one vector), and infinitely many planes (ditto, but spanned by two vectors)! What's mindblowing is that **all of them** are spaces in their own rights! ü§Ø

### plain
Most of the time, these spaces have nothing in common except the origin!

Consider for example  $\mathrm{span}\left(\left\{\left(\begin{matrix} 1 \\ 0 \\ 0 \end{matrix}\right)\right\}\right)$ and $\mathrm{span}\left(\left\{\left(\begin{matrix} 0 \\ 1 \\ 0 \end{matrix}\right)\right\}\right)$

### dialogue
But we're not really interested in these pairings (for now). What's more fascinating is spans that fundamentally describe the same space!
- Go on...
    - checkSameSpan
- Oh no...
    - checkSameSpan

### dialogue checkSameSpan
Let's suppose that $V_1 = \mathrm{span}(\{\mathbf{u}_1, \dots, \mathbf{u}_i\}), V_2 = \mathrm{span}(\{\mathbf{v}_1, \dots, \mathbf{v}_j\})$ were both subsets of $V$ (so $V_1, V_2 \subseteq V$). What should we expect to see when $V_1 \subseteq V_2$?
- Let me think about it...
    - checkSameSpan2
- Just give me the answer...
    - checkSameSpan2

### plain checkSameSpan2
Well, if and only if $V_1 \subseteq V_2$, every $\mathbf{u}_k$ is by definition of a span equivalent to some linear combination of $\{\mathbf{v}_1, \dots, \mathbf{v}_j\}$

### plain
‚òùÔ∏èü§ì So this is where the magical tool comes back to help us!

### tex
$$\textbf{Alternative Linear Combination} \\ $$

$$\mathbf{v} = \left(\begin{array}{c|c|c|c} \mathbf{u}_1 & \mathbf{u}_2 &  \dots &\mathbf{u}_k \end{array}\right) \left(\begin{array}{c} c_1 \\ c_2 \\ \vdots \\ c_k\end{array}\right) \\ $$

### plain
üë©üèª‚Äçüéì Since every $\mathbf{u}_k$ is some linear combination of $\{\mathbf{v}_1, \dots, \mathbf{v}_j\}$, we can write 
$\mathbf{u}_k = \left(\begin{array}{c|c|c|c} \mathbf{v}_1 & \mathbf{v}_2 &  \dots &\mathbf{v}_j \end{array}\right) \left(\begin{array}{c} c_{1,k} \\ c_{2,k} \\ \vdots \\ c_{j,k}\end{array}\right)$

Hence by block multiplication, we can also write
$$\left(\begin{array}{c|c|c|c} \mathbf{v}_1 & \mathbf{v}_2 &  \dots &\mathbf{v}_j \end{array}\right) \left(\begin{array}{cccc} c_{1,1} & c_{1,2} & \dots & c_{1,i} \\ c_{2,1} & c_{2,2} & \dots & c_{2, i} \\ \vdots & \vdots & \ddots & \vdots \\ c_{j,1} & c_{j,2} & \dots & c_{j,i}\end{array}\right) = \left(\begin{array}{c|c|c|c} \mathbf{u}_1 & \mathbf{u}_2 &  \dots &\mathbf{u}_i \end{array}\right)$$

### mcq
Now! What does this look like to you?
* Multiple matrix-vector equations
* A matrix equation
- I don't know, aren't you supposed to tell me?

### plain 
‚òùÔ∏èü§ì *minecraft villager noises*... there are ackchually multiple ways to view this equation.

### dialogue

But it's more interesting to consider the equation as the concatenation of multiple matrix-vector equations, because it means we MUST be able to solve 
for every $c_{a,b}$ by considering the RREF of
$\left(\begin{array}{c|c|c|c|c|c|c|c} \mathbf{v}_1 & \mathbf{v}_2 &  \dots &\mathbf{v}_j & \mathbf{u}_1 & \mathbf{u}_2 &  \dots &\mathbf{u}_i \end{array}\right)$
- **PAUSE**. Why must solution(s) exist for $c_{a,b}$?
    - explainPremise
- Cool!
    - endingQuestions
    
### plain explainPremise
Remember that we stipulated $V_1\subseteq V_2$ where $V_1 = \mathrm{span}(\{\mathbf{u}_1, \dots, \mathbf{u}_i\}), V_2 = \mathrm{span}(\{\mathbf{v}_1, \dots, \mathbf{v}_j\})$. If, let's say, $\mathbf{u}_1$ weren't a linear combination of the vectors $\{\mathbf{v}_1, \dots, \mathbf{v}_j\}$, then clearly there exists a vector in $V_1$ that is not in $V_2$.


### plain endingQuestions
Wonderful. Now let's test your understanding again:

### mcq
If two sets of vectors have the same cardinality ("size"), must their spans describe the same vector spaces?
- Yes
* No

### plain
No. Remember that not all vector spaces are built the same!

### mcq
Suppose $V_1$ and $V_2$ were as before. Then is it possible to have multiple solutions to
$\mathbf{u}_k = \left(\begin{array}{c|c|c|c} \mathbf{v}_1 & \mathbf{v}_2 &  \dots &\mathbf{v}_j \end{array}\right) \left(\begin{array}{c} c_{1,k} \\ c_{2,k} \\ \vdots \\ c_{j,k}\end{array}\right)$?

* Yes
- No

### plain
Yes. (Sneak peek into Chapter 3.5 and beyond)

### mcq
Suppose further $V_1 \subset V_2$. Can I find constants for all $\mathbf{v}_i$ to demonstrate that $\mathbf{v}_i$ is a linear combination of $\{\mathbf{u}_1, \dots, \mathbf{u}_i\}$? (Note that I swapped the roles of $\mathbf{u}$ and $\mathbf{v}$!)

- Yes
* No

### plain
Nope! No you can't.

### mcq
Suppose instead $V_1 = \mathbb{R}^n$. What should I expect to see?
- For some $1 \leq k \leq i$, $\mathbf{u}_k$ cannot be expressed as the linear combination of vectors in $\mathbb{R}^n$. 
* $i \geq n$
- For every vector $\mathbf{v} \in \mathbb{R}^n$, there exist unique $c_1, \dots, c_i$ such that $\mathbf{v} = c_1\mathbf{u}_1 + \dots + c_i \mathbf{u}_i$
* Every vector in $\mathbb{R}^n$ can be expressed as a linear combination of $\{\mathbf{u}_1, \dots, \mathbf{u}_i\}$.

## Subspaces
### plain
Let's continue tying loose ends.

### dialogue
Remember when I said that not all (vector) spaces are built the same?
- Yeah, what's up with that?
    - refresherSpace
- Err... what?
    - refresherSpace

### dialogue refresherSpace
Recall in our previous conversations I mentioned that even in Euclidean 3D space, you have infinitely many lines (1D) and planes (2D) passing through the origin, all of which can **span** many different directions.
- Yep...
    - continueSubspace

### plain continueSubspace
But one thing is common about all of them: These are bona fide vector spaces embedded in another vector space! In other words, they are **subspaces**.

### plain
To put it formally, let $V$ be a vector space. A set $U$ is a subspace of $V$ iff $U \subseteq V$ AND $U$ is a vector space.

### dialogue
One of the most common subspaces we'll learn about is **solution spaces**!
- ... the what now?
    - solnSpaces
    
### plain solnSpaces
Sorry, got ahead of myself there.
Let's phrase it like this. Consider a system $\mathbf{Ax} = \mathbf{0}$.

### plain
Then its solutions would look something like this:
$\{c_1 \mathbf{x}_1 + c_2 \mathbf{x}_2 + \dots + c_k \mathbf{x}_k \mid c_1, \dots, c_n \in \mathbb{R} \}$ where $\mathbf{x}_1, \dots, \mathbf{x}_k \in \mathbb{R}^n$ were indiviudally solutions to $\mathbf{Ax} = \mathbf{0}$.

(Think about it: for any $\mathbf{A} \mathbf{x}_i = \mathbf{0}$ and $\mathbf{A} \mathbf{x}_j = \mathbf{0}$, then $\mathbf{A} (a\mathbf{x}_i + b\mathbf{x}_j) = \mathbf{0}$)

### dialogue
Doesn't that look super familiar to you?
- Wait, that's the span of $\{\mathbf{x}_1, \dots, \mathbf{x}_n\}$!
    - solnSpaceIsSpan
    
### dialogue
Exactly! Hence, **for homogeneous systems**, we also call the solution set $\{c_1 \mathbf{x}_1 + c_2 \mathbf{x}_2 + \dots + c_k \mathbf{x}_k \mid c_1, \dots, c_n \in \mathbb{R} \}$ the solution **space** of $\mathbf{Ax} = \mathbf{0}$.
- What about inhomogeneous systems?
    - inHomSystems

### plain inHomSystems
ü•∫ Well... they can never contain the zero vector, and so they can't be vector spaces...

### plain
That doesn't seem QUITE right though. They do still represent regions of space in their own right, don't they?

### plain
Fortunately, we can apply a similar logic as we did for **solution spaces**.

### dialogue
Now consider a system $\mathbf{Ax} = \mathbf{b}$.
For ANY $\mathbf{A} \mathbf{x}_i = \mathbf{b}$ and $\mathbf{A} \mathbf{x}_j = \mathbf{b}$, then $\mathbf{A} (\mathbf{x}_i - \mathbf{x}_j) = \mathbf{0}$.
- What on earth are you cooking?
    - cooking
    
### plain cooking
Let me cook!

### plain
Don't you see that we've already found something <u>like</u> a **solution space** hiding within the solutions to $\mathbf{Ax} = \mathbf{b}$?

### plain
And it would seem that this **quasi-solution space** doesn't have a dependence on the value of $\mathbf{b}$.

### dialogue
What if... the solution set for $\mathbf{Ax} = \mathbf{b}$ is actually the solution space for $\mathbf{Ax} = \mathbf{0}$, shifted up by a chosen vector (in this case, $\mathbf{b}$)?
- brain.exe has stopped working
    - affineSpace
- You are a genius!
    - affineSpace
- I knew that all along üòé
    - affineSpace
    
### plain
It's true! While they **cannot** be called vector spaces, these spaces nevertheless pop up frequently enough that we give them their own name: **affine spaces**.

## plain
There are so many more spaces (vector or otherwise) that you will come across as you continue journeying through linear algebra.
For example, you have the **zero vector space**, which only contains the **zero** vector, the vector space of polynomials (up to degree n), and even the dual- oh wait, I'm not supposed to tell you about any non-Euclidean spaces yet :P

### plain 
In any case, this is the just the bare minimum that you should know for now. 
I encourage you to think more about the geometric meaning of vectors and matrices, and I'm excited to see what you'll be able to cook up as the weeks go by in this course!